{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMTPQfs8SZdliPRguCtmAtw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsrrenCodes/webscrapingprojects/blob/main/StackOverFlowQuestionsScrapperV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UykQGKGsPWK_"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "AtiGv4rnPXfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xfhIujm4PXog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    combined_data = []  # Create an empty list to store all the data from different pages\n",
        "\n",
        "    #Choose the number of pages you want to scrape. In this example, i chose 1 to 2. Edit the range paremeters accordingly.\n",
        "    for x in range(1,4):\n",
        "\n",
        "        list1=get_questions(\"flask\",x)\n",
        "\n",
        "        #extend data from list1 into combined_data\n",
        "        combined_data.extend(list1)\n",
        "\n",
        "    #Using pandas to convert my information into a dataframe and subsequently a csv file.\n",
        "    df=pd.DataFrame(combined_data)\n",
        "    df.to_csv(\"stackoverflow.csv\")"
      ],
      "metadata": {
        "id": "dFB4ezzJPbGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OG6z76NjVav1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_questions(tag=\"category\",page=\"the number of pages to scrape\")\n",
        "\n",
        "def get_questions(tag,page):\n",
        "    questionlist=[]\n",
        "    baseurl=f\"https://stackoverflow.com/questions/tagged/{tag}?tab=newest&page={page}&pagesize=50\"\n",
        "    questions= get_html(baseurl)\n",
        "    for question in questions:\n",
        "        database={\n",
        "            \"link\":\"https://stackoverflow.com\" + get_link(question),\n",
        "            \"title\":get_title(question),\n",
        "            \"time\":get_time(question),\n",
        "\n",
        "\n",
        "        }\n",
        "        questionlist.append(database)\n",
        "\n",
        "    return questionlist"
      ],
      "metadata": {
        "id": "BI0UCTnxPbIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_html(url):\n",
        "    #Enter your own useragent to avoid server ip ban\n",
        "    useragent={\"User-Agent\":\"https://user-agents.net/string/mozilla-5-0-windows-nt-10-0-win64-x64-applewebkit-537-36-khtml-like-gecko-chrome-95-0-4638-69-safari-537-36-ruxitsynthetic-1-0-v8351500394493611783-t3891685320802505868-ath1fb31b7a-altpriv-cvcv-2-smf-0\"}\n",
        "    r=requests.get(url,headers=useragent)\n",
        "    soup=BeautifulSoup(r.text,\"html.parser\")\n",
        "    questions=soup.find_all(\"div\",class_=\"s-post-summary--content\")\n",
        "    return questions"
      ],
      "metadata": {
        "id": "MPUJJX_6PbKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title(doc):\n",
        "    #Get title function\n",
        "    try:\n",
        "          title=doc.find(\"h3\",class_=\"s-post-summary--content-title\").text\n",
        "\n",
        "    except(AttributeError):\n",
        "          title=\"No title\"\n",
        "\n",
        "    return title"
      ],
      "metadata": {
        "id": "n97ClaLUPYBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_link(doc):\n",
        "    #Get link function\n",
        "    try:\n",
        "        link=doc.find(\"a\",class_=\"s-link\").get('href')\n",
        "\n",
        "    except(AttributeError):\n",
        "        link=\"No link avaliable\"\n",
        "\n",
        "    return link"
      ],
      "metadata": {
        "id": "l3IvdWmFPYEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time(doc):\n",
        "    #Get time function\n",
        "    try:\n",
        "        time=doc.find(\"time\",class_=\"s-user-card--time\").find(\"span\")[\"title\"]\n",
        "    except(AttributeError):\n",
        "        time=\"Time not avaliable\"\n",
        "\n",
        "    return time"
      ],
      "metadata": {
        "id": "omfO1ixBPYG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "JORN8CKBPYJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
